#ifndef _FIM_GEMV_KERNELS_FIMK_
#define _FIM_GEMV_KERNELS_FIMK_

#define PREPARE_KERNEL 1
#define PARK_IN 1
#define CHANGE_SB_HAB 1
#define PROGRAM_CRF 1
#define COMPUTE_GEMV 1
#define CHANGE_HAB_SB 1
#define PARK_OUT 1
#ifdef TARGET
#define INTEGRAL_SUM 1
#endif
__global__ void gemv_fim_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                              volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                              volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                              int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile, int output_dim,
#ifdef EMULATOR
                              FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                              uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif
    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, EVEN_BANK,
                                   offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, ODD_BANK,
                                   offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset, fbi->num_grf);
            change_fim_mode(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_64cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                       volatile uint8_t* __restrict__ fim_weight,
                                       volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                       volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                       int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                       int output_dim,
#ifdef EMULATOR
                                       FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                       uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_64cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_64cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_64cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            ODD_BANK, offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_64cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                         fbi->num_grf);
            change_fim_mode_64cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_64cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_1cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                                      volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                      volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                      int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                      int output_dim,
#ifdef EMULATOR
                                      FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                      uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_1cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_1cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_1cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           ODD_BANK, offset);
            }

            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_1cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                        fbi->num_grf);
            change_fim_mode_1cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_1cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    frd_size[0] = g_ridx[hipBlockIdx_x];
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

#ifdef EMULATOR
/* RA13 and RA12 is swapped in Aquabolt-XL core-die, we need to emulate this behavior in emulator mode */
/* 0x17ff : RA12<->RA13 swapped address in vega20 memory map */
#define HAB_ROW_ADDR 0x17ff
#define SB_ROW_ADDR 0x1fff

#else /* TARGET */

#define HAB_ROW_ADDR 0x27ff
#define SB_ROW_ADDR 0x2fff

#endif /* EMULATOR */

__global__ void gemv_fim_64cu_64th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                        volatile uint8_t* __restrict__ fim_weight,
                                        volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                        volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                        int batch_dim, int n_memory_tile, int n_compute_tile, int n_out_tile,
                                        int output_dim,
#ifdef EMULATOR
                                        FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                        uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#ifdef PREPARE_KERNEL
    int num_grf = 8;
    int num_bg = 4;
    int num_ba = 4;
    int num_col = 32;
    int trans_size = 32;
    int even_row, odd_row, row, col, loc;
    int ch = hipBlockIdx_x;
    int w_idx = hipThreadIdx_x % 2;
    int gidx = hipThreadIdx_x / 2;
    uint64_t offset = w_idx * 0x10;
    uint64_t addr;
#endif

#if PARK_IN
    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, 0, 0);
        R_CMD(&fim_weight[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if CHANGE_SB_HAB
    if (hipThreadIdx_x < 2) {
        /* change SB mode to HAB mode */
        addr = addr_gen(ch, 0, 2, 0, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 2, 1, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 0, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 1, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if PROGRAM_CRF
    if (hipThreadIdx_x < (crf_size >> 4)) {
        addr = addr_gen(hipBlockIdx_x, 0, 0, 1, 0x3fff, 0x4 + gidx);
        W_CMD_R(&fim_ctr[addr + offset], crf_binary + (hipThreadIdx_x << 4));
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if COMPUTE_GEMV
    if (hipThreadIdx_x < 16) {
        /* change HAB mode to HAB_FIM mode */
        for (int b_idx = 0; b_idx < batch_dim; b_idx++) {
            for (int o_idx = 0; o_idx < n_out_tile; o_idx++) {
                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_to_hab_fim + w_idx * 16);
                B_CMD(1);

                uint64_t i_offset = b_idx * n_memory_tile * num_grf;
                int r_offset = (o_idx * n_memory_tile) / 2;

                for (int i_idx = 0; i_idx < n_compute_tile; i_idx += 2) {
                    /* write grf_A from WRIO */
                    uint64_t i_addr = (i_offset + (i_idx * num_grf + gidx)) * trans_size;
                    addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    R_CMD(&fim_ctr[addr + offset]);
                    B_CMD(1);

                    even_row = ((i_idx / 2) + r_offset) * 2;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }

                for (int i_idx = 1; i_idx < n_compute_tile; i_idx += 2) {
                    uint64_t i_addr = (i_offset + (i_idx * num_grf + gidx)) * trans_size;
                    addr = addr_gen(ch, 0, 0, 1, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    R_CMD(&fim_ctr[addr + offset]);
                    B_CMD(1);

                    even_row = ((i_idx / 2) + r_offset) * 2;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }
                loc = b_idx * n_out_tile * num_grf + o_idx * num_grf + gidx;
                row = loc / num_col;
                col = loc % num_col;

                // pipeline delay
                // FIX : If alu is in operation, NOP should be added.
                addr = addr_gen(ch, 0, 0, 1, row, col);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                B_CMD(1);

                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_fim_to_hab + w_idx * 16);
                B_CMD(1);
            }
        }
    }
    B_CMD(0);
#endif

#if CHANGE_HAB_SB
    if (hipThreadIdx_x < 4) {
        /* change HAB mode to SB mode */
        addr = addr_gen(ch, 0, 0, gidx, SB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        R_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if PARK_OUT
    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, 0, 0);
        R_CMD(&fim_weight[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#ifdef EMULATOR
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    int bg = hipThreadIdx_x / 16;
    int ba = (((hipThreadIdx_x / 8) % 2) * 2) + 1;
    int out_idx = (hipBlockIdx_x * 64) + hipThreadIdx_x;
    int out_offset;
    half t_output;

    for (int i = 0; i < batch_dim * n_out_tile; i++) {
        if (out_idx < output_dim) {
            t_output = 0;
            row = i / 4;
            col = hipThreadIdx_x % 8 + ((i % 4) * 8);
            addr = addr_gen(ch, 0, bg, ba, row, col);
            for (int j = 0; j < 16; j++) {
                t_output += ((half*)fim_gemv_tmp_buffer)[(addr >> 1) + j];
            }
            out_offset = i * output_dim + out_idx;
            ((half*)output)[out_offset] = (half)is_gemv_add * ((half*)output)[out_offset] + t_output;
        }
    }
#endif
}

__device__ __global__ void mac_test_64th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                              volatile uint8_t* __restrict__ fim_weight,
                                              volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                              volatile uint8_t* __restrict__ fim_input,
                                              volatile uint8_t* __restrict__ output, int batch_dim, int n_memory_tile,
                                              int n_compute_tile, int n_out_tile, int output_dim,
#ifdef EMULATOR
                                              FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                              uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
    int num_bg = 4;
    int num_ba = 4;
    int w_idx = hipThreadIdx_x % 2;
    int gidx = hipThreadIdx_x / 2;
    uint64_t offset = w_idx * 0x10;
    uint64_t addr;
    int ch = hipBlockIdx_x;

    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, 0, 0);
        R_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);

    if (hipThreadIdx_x < 2) {
        /* change SB mode to HAB mode */
        addr = addr_gen(ch, 0, 2, 0, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 2, 1, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 0, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 1, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);

        /* set crf binary */
        addr = addr_gen(ch, 0, 0, 1, 0x3fff, 0x4);
        W_CMD_R(&fim_ctr[addr + offset], crf_binary + hipThreadIdx_x * 16);
        B_CMD(1);
    }
    B_CMD(0);

    int even_row, odd_row;
    if (hipThreadIdx_x < 16) {
        /* change HAB mode to HAB_FIM mode */
        for (int b = 0; b < batch_dim; b++) {
            for (int j = 0; j < n_out_tile; j++) {
                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_to_hab_fim + w_idx * 16);
                B_CMD(1);

                for (int i_idx = 0; i_idx < n_compute_tile; i_idx += 2) {
                    /* write grf_A from WRIO */
                    addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], fim_input + ((i_idx * 8 + gidx) * 32 + (w_idx * 16)));
                    B_CMD(1);
                    B_CMD(1);

                    even_row = (i_idx >> 1) << 1;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }

                for (int i_idx = 1; i_idx < n_compute_tile; i_idx += 2) {
                    addr = addr_gen(ch, 0, 0, 1, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], fim_input + (i_idx * 8 + gidx) * 32 + (w_idx * 16));
                    B_CMD(1);
                    B_CMD(1);

                    even_row = (i_idx >> 1) << 1;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }

                // pipeline delay
                // FIX : If alu is in operation, NOP should be added.
                addr = addr_gen(ch, 0, 0, 1, 0, gidx);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                B_CMD(1);

                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_fim_to_hab + w_idx * 16);
                B_CMD(1);
            }
        }
    }
    B_CMD(0);

    if (hipThreadIdx_x < 2) {
        /* change HAB mode to SB mode */
        addr = addr_gen(ch, 0, 0, 0, 0x2fff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 1, 0x2fff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);

    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, 0, 0);
        R_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);

    int bg = hipThreadIdx_x >> 4;
    int ba = (((hipThreadIdx_x >> 3) % 2) << 1) + 1;
    int out_idx = (hipBlockIdx_x << 6) + hipThreadIdx_x;
    int row;
    int col;

    half t_output;

    for (int i = 0; i < batch_dim * n_out_tile; i++) {
        if (out_idx < output_dim) {
            t_output = 0;
            row = i % 4;
            col = hipThreadIdx_x % 8 + ((i % 4) << 3);
            addr = addr_gen(ch, 0, bg, ba, row, col);
            for (int j = 0; j < 16; j++) {
                t_output += ((half*)fim_gemv_tmp_buffer)[(addr >> 1) + j];
            }
            ((half*)output)[out_idx] = (half)is_gemv_add * ((half*)output)[out_idx] + t_output;
        }
    }
    B_CMD(0);
}
#endif /* _FIM_GEMV_KERNELS_FIMK_ */
