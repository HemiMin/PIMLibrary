#ifndef _FIM_GEMV_KERNELS_FIMK_
#define _FIM_GEMV_KERNELS_FIMK_

#define PREPARE_KERNEL 1
#define PARK_IN 1
#define CHANGE_SB_HAB 1
#define PROGRAM_CRF 1
#define COMPUTE_GEMV 1
#define CHANGE_HAB_SB 1
#define PARK_OUT 1
#ifdef TARGET
#define INTEGRAL_SUM 1
#endif

__global__ void gemv_fim_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                              volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                              volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                              int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile, int output_dim,
#ifdef EMULATOR
                              FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                              uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif
    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, EVEN_BANK,
                                   offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, ODD_BANK,
                                   offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset, fbi->num_grf);
            change_fim_mode(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_64cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                       volatile uint8_t* __restrict__ fim_weight,
                                       volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                       volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                       int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                       int output_dim,
#ifdef EMULATOR
                                       FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                       uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_64cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_64cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_64cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            ODD_BANK, offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_64cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                         fbi->num_grf);
            change_fim_mode_64cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_64cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_1cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                                      volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                      volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                      int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                      int output_dim,
#ifdef EMULATOR
                                      FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                      uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_1cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_1cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_1cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           ODD_BANK, offset);
            }

            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_1cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                        fbi->num_grf);
            change_fim_mode_1cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_1cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    frd_size[0] = g_ridx[hipBlockIdx_x];
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    integral_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

#ifdef EMULATOR
/* RA13 and RA12 is swapped in Aquabolt-XL core-die, we need to emulate this behavior in emulator mode */
/* 0x17ff : RA12<->RA13 swapped address in vega20 memory map */
#define HAB_ROW_ADDR 0x17ff
#define SB_ROW_ADDR 0x1fff

#else /* TARGET */

#define HAB_ROW_ADDR 0x27ff
#define SB_ROW_ADDR 0x2fff

#endif /* EMULATOR */

__global__ void gemv_fim_64cu_64th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                        volatile uint8_t* __restrict__ fim_weight,
                                        volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                        volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                        int batch_dim, int n_memory_tile, int n_compute_tile, int n_out_tile,
                                        int output_dim,
#ifdef EMULATOR
                                        FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                        uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    int num_col = 32;
    int num_bg = 4;
    int num_ba = 4;
    int num_grf = 8;
    int loc, row, col;

    int num_out_tile = n_out_tile;

    int gidx = hipThreadIdx_x >> 1;
    uint64_t offset = (hipThreadIdx_x % 2) << 4;
    uint64_t addr;
#endif
    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    /* num_bg * num_ba * 2 */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(hipBlockIdx_x, 0, gidx >> 2, gidx % num_ba, 0, 0);
        R_CMD(&fim_ctr[addr + offset]);
    }
    B_CMD(0);
#endif

#if CHANGE_SB_HAB
    if (hipThreadIdx_x < 2) {
        addr = addr_gen(hipBlockIdx_x, 0, 2, gidx, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(hipBlockIdx_x, 0, 2, gidx + 1, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(hipBlockIdx_x, 0, 0, gidx, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(hipBlockIdx_x, 0, 0, gidx + 1, HAB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if PROGRAM_CRF
    if (hipThreadIdx_x < (crf_size >> 4)) {
        addr = addr_gen(hipBlockIdx_x, 0, 0, 1, 0x3fff, 0x4 + gidx);
        W_CMD_R(&fim_ctr[addr + offset], crf_binary + (hipThreadIdx_x << 4));
    }
    B_CMD(0);
#endif

#if COMPUTE_GEMV
    if (hipThreadIdx_x < 16 /* 2 * num_grf */) {
        for (int b_idx = 0; b_idx < batch_dim; b_idx++) {
            for (int o_idx = 0; o_idx < num_out_tile; o_idx++) {
                addr = addr_gen(hipBlockIdx_x, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_to_hab_fim + ((hipThreadIdx_x % 2) << 4));
                B_CMD(1);

                uint64_t i_offset = (b_idx * n_memory_tile << 3) + gidx;
                int r_offset = (o_idx * n_memory_tile) >> 1;

                for (int i_idx = 0; i_idx < n_compute_tile; i_idx += 2) {
                    uint64_t i_addr = (i_offset + (i_idx << 3)) << 5;
                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    B_CMD(1);

                    row = ((i_idx >> 1) + r_offset) << 1;
                    col = gidx;

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row, col);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row, col + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row, col + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row, col + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row + 1, col);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row + 1, col + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row + 1, col + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 0, row + 1, col + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }

                for (int i_idx = 1; i_idx < n_compute_tile; i_idx += 2) {
                    uint64_t i_addr = (i_offset + (i_idx << 3)) << 5;
                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    B_CMD(1);

                    row = ((i_idx >> 1) + r_offset) << 1;
                    col = gidx;

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row, col);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row, col + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row, col + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row, col + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row + 1, col);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row + 1, col + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row + 1, col + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row + 1, col + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }
                loc = b_idx * num_out_tile * num_grf + o_idx * num_grf + gidx;
                row = loc >> 5;  // loc / num_col
                col = loc % num_col;

                addr = addr_gen(hipBlockIdx_x, 0, 0, 1, row, col);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                B_CMD(1);

                addr = addr_gen(hipBlockIdx_x, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_fim_to_hab + ((hipThreadIdx_x % 2) << 4));
                B_CMD(1);
            }
        }
    }
    B_CMD(0);
#endif

#if CHANGE_HAB_SB
    if (hipThreadIdx_x < 4) {
        addr = addr_gen(hipBlockIdx_x, 0, 0, gidx, SB_ROW_ADDR, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
    }
    B_CMD(0);
#endif

#if PARK_OUT
    if (hipThreadIdx_x < num_bg * num_ba * 2) {
        addr = addr_gen(hipBlockIdx_x, 0, gidx / num_ba, gidx % num_ba, 0, 0);
        R_CMD(&fim_weight[addr + offset]);
    }
    B_CMD(0);
#endif

#ifdef EMULATOR
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if INTEGRAL_SUM
    /* TODO: verify reduce sum in Target Mode */
    int bg = hipThreadIdx_x >> 4;
    int ba = (((hipThreadIdx_x >> 3) % 2) << 1) + 1;
    int out_idx = (hipBlockIdx_x << 6) + hipThreadIdx_x;
    half t_output;
    for (int i = 0; i < batch_dim * num_out_tile; i++) {
        if (out_idx < output_dim) {
            t_output = 0;
            row = i >> 2;
            col = hipThreadIdx_x % 8 + ((i % 4) << 3);
            addr = addr_gen(hipBlockIdx_x, 0, bg, ba, row, col);
            for (int j = 0; j < 16; j++) {
                t_output += fim_gemv_tmp_buffer[addr + j];
            }
            ((half*)output)[out_idx] = (half)is_gemv_add * ((half*)output)[out_idx] + t_output;
        }
    }
#endif
}

#endif /* _FIM_GEMV_KERNELS_FIMK_ */
