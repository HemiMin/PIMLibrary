#ifndef _FIM_GEMV_KERNELS_FIMK_
#define _FIM_GEMV_KERNELS_FIMK_

#define PREPARE_KERNEL 1
#define PARK_IN 1
#define CHANGE_SB_HAB 1
#define PROGRAM_CRF 1
#define COMPUTE_GEMV 1
#define CHANGE_HAB_SB 1
#define PARK_OUT 1
#ifdef TARGET
#define REDUCE_SUM 1
#endif
__global__ void gemv_fim_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                              volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                              volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                              int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile, int output_dim,
#ifdef EMULATOR
                              FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                              uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif
    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, EVEN_BANK,
                                   offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b, ODD_BANK,
                                   offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset, fbi->num_grf);
            change_fim_mode(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if REDUCE_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    reduce_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_64cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                       volatile uint8_t* __restrict__ fim_weight,
                                       volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                       volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                       int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                       int output_dim,
#ifdef EMULATOR
                                       FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                       uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_64cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_64cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_64cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_64cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                            ODD_BANK, offset);
            }
            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_64cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                         fbi->num_grf);
            change_fim_mode_64cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_64cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_64cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if REDUCE_SUM
    /* TODO: verify reduce sum in Target Mode */
    __syncthreads();
    reduce_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_1cu_2th_fp16(volatile uint8_t* __restrict__ fim_ctr, volatile uint8_t* __restrict__ fim_weight,
                                      volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                      volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                      int batch_dim, int n_memory_tile, int n_compute_tile, int num_out_tile,
                                      int output_dim,
#ifdef EMULATOR
                                      FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                      uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#if PREPARE_KERNEL
    uint64_t offset = (hipThreadIdx_x % 2) * 0x10;
    FimBlockInfo* fbi = &vega20_fbi;
    int compute_col = 0;
#endif

    /* Radeon7(VEGA20) memory is 16GB but our target is 32GB system */
    /* so program_crf and chagne_fim_mode functions can not access to over 8GB in our system */
#if PARK_IN
    park_1cu_2th(fim_weight, offset);
#endif

#if CHANGE_SB_HAB
    change_fim_mode_1cu_2th(fim_ctr, SB_MODE, HAB_MODE, null_bst, offset);
#endif

#if PROGRAM_CRF
    program_crf_1cu_2th(fim_ctr, crf_binary, crf_size, offset);
#endif

#if COMPUTE_GEMV
    for (int b = 0; b < batch_dim; b++) {
        for (int j = 0; j < num_out_tile; j++) {
            change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, HAB_FIM_MODE, gemv_hab_to_hab_fim, offset);
            for (int i = 0; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           EVEN_BANK, offset);
            }
            for (int i = 1; i < n_compute_tile; i += 2) {
                compute_gemv_2bank_1cu_2th(fim_ctr, fim_weight, fim_input, n_memory_tile, num_out_tile, i, j, b,
                                           ODD_BANK, offset);
            }

            compute_col = b * num_out_tile * fbi->num_grf_B + j * fbi->num_grf_B;
            add_transaction_all_1cu_2th(fim_gemv_tmp_buffer, true, 0, 1, 0, compute_col, null_bst, offset,
                                        fbi->num_grf);
            change_fim_mode_1cu_2th(fim_ctr, HAB_FIM_MODE, HAB_MODE, gemv_hab_fim_to_hab, offset);
        }
    }
#endif

#if CHANGE_HAB_SB
    change_fim_mode_1cu_2th(fim_ctr, HAB_MODE, SB_MODE, null_bst, offset);
#endif

#if PARK_OUT
    park_1cu_2th(fim_weight, offset);
#endif

#ifdef EMULATOR
    __syncthreads();
    frd_size[0] = g_ridx[hipBlockIdx_x];
#endif

#if REDUCE_SUM
    /* TODO: verify reduce sum in Target Mode */
    reduce_sum_for_gemv_gpu((void*)output, (void*)fim_gemv_tmp_buffer, output_dim, fbi->num_out_per_grf);
#endif
}

__global__ void gemv_fim_64cu_64th_fp16(volatile uint8_t* __restrict__ fim_ctr,
                                        volatile uint8_t* __restrict__ fim_weight,
                                        volatile uint8_t* __restrict__ fim_gemv_tmp_buffer,
                                        volatile uint8_t* __restrict__ fim_input, volatile uint8_t* __restrict__ output,
                                        int batch_dim, int n_memory_tile, int n_compute_tile, int n_out_tile,
                                        int output_dim,
#ifdef EMULATOR
                                        FimMemTraceData* fmtd16, int* frd_size, int mt_width,
#endif
                                        uint8_t* crf_binary, int crf_size, int is_gemv_add)
{
#ifdef EMULATOR
    g_fba = (uint64_t)fim_ctr;
    g_fmtd16 = fmtd16;
    g_ridx[hipBlockIdx_x] = 0;
    m_width = mt_width;
    __syncthreads();
#endif

#ifdef PREPARE_KERNEL
    int num_grf = 8;
    int num_bg = 4;
    int num_ba = 4;
    int num_col = 32;
    int trans_size = 32;
    int even_row, odd_row, row, col, loc;
    int ch = hipBlockIdx_x;
    int w_idx = hipThreadIdx_x % 2;
    int gidx = hipThreadIdx_x / 2;
    uint64_t offset = w_idx * 0x10;
    uint64_t addr;
#endif

#if PARK_IN
    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, (1 << 13), 0);
        W_CMD(&fim_ctr[addr + offset]);
    }
    B_CMD(1);
    B_CMD(0);
#endif

#if CHANGE_SB_HAB
    if (hipThreadIdx_x < 2) {
        /* change SB mode to HAB mode */
        addr = addr_gen(ch, 0, 2, 0, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 2, 1, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 0, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
        addr = addr_gen(ch, 0, 0, 1, 0x27ff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if PROGRAM_CRF
    if (hipThreadIdx_x < (crf_size >> 4)) {
        addr = addr_gen(hipBlockIdx_x, 0, 0, 1, 0x3fff, 0x4 + gidx);
        W_CMD_R(&fim_ctr[addr + offset], crf_binary + (hipThreadIdx_x << 4));
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if COMPUTE_GEMV
    if (hipThreadIdx_x < 16) {
        /* change HAB mode to HAB_FIM mode */
        for (int b_idx = 0; b_idx < batch_dim; b_idx++) {
            for (int o_idx = 0; o_idx < n_out_tile; o_idx++) {
                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_to_hab_fim + offset);
                B_CMD(1);

                uint64_t i_offset = b_idx * n_memory_tile * num_grf;
                int r_offset = (o_idx * n_memory_tile) / 2;

                for (int i_idx = 0; i_idx < n_compute_tile; i_idx += 2) {
                    /* write grf_A from WRIO */
                    uint64_t i_addr = (i_offset + (i_idx * num_grf + gidx)) * trans_size;
                    addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    //                    R_CMD(&fim_ctr[addr + offset]);
                    B_CMD(1);
                    B_CMD(1);

                    even_row = ((i_idx / 2) + r_offset) * 2;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 0, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }

                for (int i_idx = 1; i_idx < n_compute_tile; i_idx += 2) {
                    uint64_t i_addr = (i_offset + (i_idx * num_grf + gidx)) * trans_size;
                    addr = addr_gen(ch, 0, 0, 1, 0x3fff, 0x8 + gidx);
                    W_CMD_R(&fim_ctr[addr + offset], &fim_input[i_addr + offset]);
                    //                    R_CMD(&fim_ctr[addr + offset]);
                    B_CMD(1);
                    B_CMD(1);

                    even_row = ((i_idx / 2) + r_offset) * 2;
                    odd_row = even_row + 1;

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, even_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 8);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 16);
                    R_CMD(&fim_weight[addr + offset]);

                    addr = addr_gen(ch, 0, 0, 1, odd_row, gidx + 24);
                    R_CMD(&fim_weight[addr + offset]);
                    B_CMD(1);
                }
                loc = b_idx * n_out_tile * num_grf + o_idx * num_grf + gidx;
                row = loc / num_col;
                col = loc % num_col;

                // pipeline delay
                // FIX : If alu is in operation, NOP should be added.
                addr = addr_gen(ch, 0, 0, 1, row, col);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                W_CMD(&fim_gemv_tmp_buffer[addr + offset]);
                B_CMD(1);

                addr = addr_gen(ch, 0, 0, 0, 0x3fff, 0x0);
                W_CMD_R(&fim_ctr[addr + offset], gemv_hab_fim_to_hab + offset);
                B_CMD(1);
            }
        }
    }
    B_CMD(0);
#endif

#if CHANGE_HAB_SB
    if (hipThreadIdx_x < 4) {
        /* change HAB mode to SB mode */
        addr = addr_gen(ch, 0, 0, gidx, 0x2fff, 0x1f);
        W_CMD(&fim_ctr[addr + offset]);
        B_CMD(1);
    }
    B_CMD(0);
#endif

#if PARK_OUT
    /* park */
    if (hipThreadIdx_x < 32) {
        addr = addr_gen(ch, 0, gidx / num_ba, gidx % num_ba, (1 << 13), 0);
        W_CMD(&fim_ctr[addr + offset]);
    }
    B_CMD(1);
    B_CMD(0);
#endif

#ifdef EMULATOR
    if (hipBlockIdx_x == 0 && hipThreadIdx_x == 0) {
        frd_size[0] = g_ridx[0];
    }
#endif

#if REDUCE_SUM
    int out_per_tile = 4096;
    int bg = hipThreadIdx_x / 16;
    int ba = (((hipThreadIdx_x / 8) % 2) * 2) + 1;
    int t_idx = (hipBlockIdx_x * 64) + hipThreadIdx_x;
    int out_idx;
    int out_offset;
    int li;
    half t_output;

    for (int bi = 0; bi < batch_dim; bi++) {
        for (int oi = 0; oi < n_out_tile; oi++) {
            out_idx = oi * out_per_tile + t_idx;
            if (out_idx < output_dim) {
                li = bi * n_out_tile + oi;
                row = li / 4;
                col = hipThreadIdx_x % 8 + ((li % 4) * 8);
                addr = addr_gen(ch, 0, bg, ba, row, col);
                t_output = 0;
                for (int ti = 0; ti < 16; ti++) {
                    t_output += ((half*)fim_gemv_tmp_buffer)[(addr >> 1) + ti];
                }
                out_offset = bi * output_dim + out_idx;
                if (is_gemv_add)
                    ((half*)output)[out_offset] += t_output;
                else
                    ((half*)output)[out_offset] = t_output;
            }
        }
    }
#endif
}

#endif /* _FIM_GEMV_KERNELS_FIMK_ */
